import json
import re
from pathlib import Path

# --- Configuration ---
INPUT_FILE = Path("../temp_models/typed_dicts.py")
BASE_TYPED_DICTS_OUTPUT = Path("../../src/multiconn_archicad/dicts/types.py")
COMMAND_TYPED_DICTS_OUTPUT = Path("../../src/multiconn_archicad/dicts/commands.py")
BASE_NAMES_FILE = Path("../schema/_base_model_names.json")
COMMAND_NAMES_FILE = Path("../schema/_command_model_names.json")


def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*=)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Gets the common header lines for a model file."""
    return [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, TypedDict, Union",
        "from typing_extensions import NotRequired",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_dependencies(source_blocks: list[str], available_definitions: set[str]) -> list[str]:
    """
    Scans source code blocks for type annotations and returns a sorted list
    of dependencies that are present in the available_definitions set.
    """
    source_content = "\n".join(source_blocks)

    # DEFINITIVE FIX: This regex is much more precise. It looks for capitalized words
    # that are used as type annotations (after a colon) or within generics (like List[...]).
    # It specifically ignores words that are part of another word (e.g., won't find 'Error' in 'ErrorItem').
    # It finds:
    #   : SomeType
    #   [SomeType]
    #   | SomeType
    #   (SomeType
    pattern = re.compile(r"[:\[\|\(]\s*([A-Z]\w+)")

    used_identifiers = set(pattern.findall(source_content))

    # The needed imports are the intersection of identifiers found and base models available.
    return sorted(list(used_identifiers.intersection(available_definitions)))


def main():
    """
    Splits a single TypedDicts file into base and command files,
    generating explicit, non-greedy imports to satisfy linters like ruff.
    """
    print("--- Starting TypedDict Splitting Process (with Precise Imports) ---")

    # 1. Load model name sets
    try:
        with open(BASE_NAMES_FILE, "r", encoding="utf-8") as f:
            base_names = set(json.load(f))
        with open(COMMAND_NAMES_FILE, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"Error: Could not find name list file. ({e})")
        return

    # 2. Read and parse the input file
    try:
        content = INPUT_FILE.read_text("utf-8")
    except FileNotFoundError:
        print(f"Error: {INPUT_FILE} not found.")
        return

    definitions_raw = re.split(r"\n\n\n", content)
    definitions = [
        block.strip() for block in definitions_raw if block.strip() and not block.strip().startswith(("from ", "###"))
    ]

    # 3. Sort definitions into base and command lists
    base_blocks = []
    command_blocks = []
    for block in definitions:
        name = get_definition_name(block)
        if name in command_names:
            command_blocks.append(block)
        else:
            base_blocks.append(block)

    print(f"Sorted TypedDicts: {len(base_blocks)} base | {len(command_blocks)} command")

    # --- Generate Base Models File ---
    base_file_content = "\n".join(get_header_lines()) + "\n\n\n" + "\n\n\n".join(sorted(base_blocks))
    BASE_TYPED_DICTS_OUTPUT.write_text(base_file_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(base_blocks)} definitions to {BASE_TYPED_DICTS_OUTPUT}")

    # --- Generate Command Models File ---
    # 4. Find all dependencies needed by the command models from the base models
    base_model_names_in_file = {get_definition_name(b) for b in base_blocks if get_definition_name(b)}
    needed_imports = find_dependencies(command_blocks, base_model_names_in_file)
    print(f"Found {len(needed_imports)} base models required by command models.")

    # 5. Assemble the final command models file
    header = get_header_lines()
    file_parts = ["\n".join(header)]

    # DEFINITIVE FIX: Only add the import block if there are imports to add.
    if needed_imports:
        # Create a nicely formatted, multi-line import statement
        import_block = "from .base_typed_dicts import (\n    " + ",\n    ".join(needed_imports) + ",\n)"
        file_parts.append(import_block)

    file_parts.extend(sorted(command_blocks))

    final_command_content = "\n\n\n".join(file_parts)
    COMMAND_TYPED_DICTS_OUTPUT.write_text(final_command_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(command_blocks)} definitions to {COMMAND_TYPED_DICTS_OUTPUT}")


if __name__ == "__main__":
    main()

import json
import re
from paths import paths

def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*=)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Creates the standard list of header import lines for the model files."""
    return [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, Union, TypeAlias",
        "from uuid import UUID",
        "from enum import Enum",
        "from pydantic import BaseModel, ConfigDict, Field, RootModel",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_cross_file_dependencies(block: str, available_base_definitions: set[str]) -> set[str]:
    """
    Scans a command model's source code block and returns a set of dependencies
    that exist in the set of available base model names.
    """
    # This regex is proven to find capitalized words used as type hints.
    pattern = re.compile(r"[:\[\|\(\s=]\s*([A-Z]\w*)")
    used_identifiers = set(pattern.findall(block))
    return used_identifiers.intersection(available_base_definitions)


def main():
    """
    Reads a single, ordered Pydantic models file and splits it into
    `types.py` and `commands.py`, preserving order and creating explicit imports.
    """
    print("--- Starting Model Splitting Process (with Explicit Imports) ---")

    # 1. Load the model name lists
    try:
        with open(paths.BASE_MODEL_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            base_names = set(json.load(f))
        with open(paths.COMMAND_MODELS_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"Error: Could not find name list file. ({e})")
        return

    # 2. Read the generated Python file
    try:
        content = paths.CLEANED_PYDANTIC_MODELS.read_text("utf-8")
    except FileNotFoundError:
        print(f"Error: {paths.CLEANED_PYDANTIC_MODELS} not found. Please run the cleaner script first.")
        return

    # 3. Split the content into definition blocks while preserving their order
    definitions_in_order = [
        block.strip()
        for block in re.split(r"\n\n\n", content)
        if block.strip() and not block.strip().startswith(("from ", "###", "#"))
    ]

    if not definitions_in_order:
        print("Error: No model definitions found in input file.")
        return

    # 4. Sort each definition block into the correct category, maintaining original relative order
    base_model_blocks = []
    command_model_blocks = []
    for block in definitions_in_order:
        name = get_definition_name(block)
        if not name:
            print(f"Warning: Could not identify name for block, placing in types.py:\n{block[:100]}...")
            base_model_blocks.append(block)
            continue

        if name in command_names:
            command_model_blocks.append(block)
        else:  # If it's not a command, it's a base type.
            base_model_blocks.append(block)

    print(f"Sorted models: {len(base_model_blocks)} base | {len(command_model_blocks)} command")

    # 5. Assemble and write the final files

    # --- Write `types.py` (Base Models) ---
    print("\nProcessing base models (types.py)...")
    base_header = "\n".join(get_header_lines())
    # The blocks are already in the correct order, no sorting is needed or wanted.
    base_content = base_header + "\n\n\n" + "\n\n\n".join(base_model_blocks)
    paths.FINAL_PYDANTIC_TYPES.parent.mkdir(parents=True, exist_ok=True)
    paths.FINAL_PYDANTIC_TYPES.write_text(base_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(base_model_blocks)} definitions to {paths.FINAL_PYDANTIC_TYPES}")

    # --- Write `commands.py` (Command Models) ---
    print("\nProcessing command models (commands.py)...")

    # Find all dependencies needed by command models from the base models
    base_model_names = {get_definition_name(b) for b in base_model_blocks if get_definition_name(b)}
    needed_imports = set()
    for block in command_model_blocks:
        needed_imports.update(find_cross_file_dependencies(block, base_model_names))

    print(f"Found {len(needed_imports)} base model dependencies required by command models.")

    command_header = "\n".join(get_header_lines())
    file_parts = [command_header]

    # Create a nicely formatted, multi-line import statement for the dependencies
    if needed_imports:
        # User wants the import path to be `.types`
        import_block = "from .types import (\n    " + ",\n    ".join(sorted(list(needed_imports))) + ",\n)"
        file_parts.append(import_block)

    # Add the command model blocks, already in their correct relative order
    file_parts.extend(command_model_blocks)

    command_content = "\n\n\n".join(file_parts)
    paths.FINAL_PYDANTIC_COMMANDS.parent.mkdir(parents=True, exist_ok=True)
    paths.FINAL_PYDANTIC_COMMANDS.write_text(command_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(command_model_blocks)} definitions to {paths.FINAL_PYDANTIC_COMMANDS}")


if __name__ == "__main__":
    main()

import json
import re
from pathlib import Path
from collections import defaultdict

# --- Configuration ---
INPUT_FILE = Path("../temp_models/typed_dicts.py")
BASE_TYPED_DICTS_OUTPUT = Path("../../src/multiconn_archicad/dicts/types.py")
COMMAND_TYPED_DICTS_OUTPUT = Path("../../src/multiconn_archicad/dicts/commands.py")
BASE_NAMES_FILE = Path("../schema/_base_model_names.json")
COMMAND_NAMES_FILE = Path("../schema/_command_model_names.json")


def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*=)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Gets the common header lines for a model file."""
    return [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, TypedDict, Union",
        "from typing_extensions import NotRequired",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_cross_file_dependencies(block: str, available_base_definitions: set[str]) -> set[str]:
    """
    Scans a single source code block and returns a set of dependencies
    that are present in the available_base_definitions set.
    """
    pattern = re.compile(r"[:\[\|\(\s=]\s*([A-Z]\w*)")
    used_identifiers = set(pattern.findall(block))
    return used_identifiers.intersection(available_base_definitions)


def remove_unused_imports(content: str) -> str:
    """
    Removes unused typing imports from the header of the file content.
    """
    lines = content.split('\n')
    typing_imports_line_index = -1
    for i, line in enumerate(lines):
        if "from typing import" in line:
            typing_imports_line_index = i
            break

    if typing_imports_line_index == -1:
        return content

    body_content = "\n".join(lines[typing_imports_line_index + 1:])

    used_imports = []
    if re.search(r"\bAny\b", body_content): used_imports.append("Any")
    if re.search(r"\bList\b", body_content): used_imports.append("List")
    if re.search(r"\bLiteral\b", body_content): used_imports.append("Literal")
    if re.search(r"\bTypedDict\b", body_content): used_imports.append("TypedDict")
    if re.search(r"\bUnion\b", body_content): used_imports.append("Union")

    if used_imports:
        lines[typing_imports_line_index] = f"from typing import {', '.join(sorted(used_imports))}"
    else:
        # If no typing imports are used, remove the line
        del lines[typing_imports_line_index]

    return "\n".join(lines)


def main():
    """
    Splits a single TypedDicts file into base and command files,
    PRESERVING the original definition order.
    """
    print("--- Starting TypedDict Splitting Process (Preserving Original Order) ---")

    # 1. Load model name sets
    try:
        BASE_NAMES_FILE.parent.mkdir(parents=True, exist_ok=True)
        if not BASE_NAMES_FILE.exists():
            with open(BASE_NAMES_FILE, 'w', encoding='utf-8') as f: json.dump([], f)
        if not COMMAND_NAMES_FILE.exists():
            with open(COMMAND_NAMES_FILE, 'w', encoding='utf-8') as f: json.dump([], f)

        with open(BASE_NAMES_FILE, "r", encoding="utf-8") as f:
            base_names = set(json.load(f))
        with open(COMMAND_NAMES_FILE, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Warning: Could not load name list files. Defaulting to base. ({e})")
        base_names = set()
        command_names = set()

    # 2. Read and parse the input file, preserving the block order
    try:
        content = INPUT_FILE.read_text("utf-8")
    except FileNotFoundError:
        print(f"Error: {INPUT_FILE} not found.")
        return

    definitions_in_order = [
        block.strip() for block in re.split(r"\n\n\n", content)
        if block.strip() and not block.strip().startswith(("from ", "###"))
    ]

    if not definitions_in_order:
        print("Error: No definitions found in the input file.")
        return

    all_definition_names = {get_definition_name(b) for b in definitions_in_order if get_definition_name(b)}
    if not base_names and not command_names:
        base_names = all_definition_names

    # 3. Categorize definitions into two lists, maintaining their original relative order
    base_blocks = []
    command_blocks = []
    for block in definitions_in_order:
        name = get_definition_name(block)
        if name in command_names:
            command_blocks.append(block)
        else: # Default to base
            base_blocks.append(block)

    print(f"Found {len(definitions_in_order)} total definitions.")
    print(f"Categorized into: {len(base_blocks)} base | {len(command_blocks)} command")

    # --- Generate Base Models File (`types.py`) ---
    print("\nProcessing base models...")
    base_file_content = "\n".join(get_header_lines()) + "\n\n\n" + "\n\n\n".join(base_blocks)
    final_base_content = remove_unused_imports(base_file_content)
    BASE_TYPED_DICTS_OUTPUT.write_text(final_base_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(base_blocks)} definitions to {BASE_TYPED_DICTS_OUTPUT}")

    # --- Generate Command Models File (`commands.py`) ---
    print("\nProcessing command models...")
    base_model_names_in_file = {get_definition_name(b) for b in base_blocks if get_definition_name(b)}

    needed_imports = set()
    for block in command_blocks:
        needed_imports.update(find_cross_file_dependencies(block, base_model_names_in_file))

    print(f"Found {len(needed_imports)} base models required by command models.")

    # Assemble the final file content
    header_content = "\n".join(get_header_lines())
    file_parts = [header_content]

    if needed_imports:
        sorted_imports = sorted(list(needed_imports))
        import_block = "from .types import (\n    " + ",\n    ".join(sorted_imports) + ",\n)"
        file_parts.append(import_block)

    # Add the command blocks, which are already in the correct relative order
    file_parts.extend(command_blocks)

    final_command_content = "\n\n\n".join(file_parts)
    final_command_content_cleaned = remove_unused_imports(final_command_content)
    COMMAND_TYPED_DICTS_OUTPUT.write_text(final_command_content_cleaned + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(command_blocks)} definitions to {COMMAND_TYPED_DICTS_OUTPUT}")


if __name__ == "__main__":
    main()
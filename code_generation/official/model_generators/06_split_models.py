# code_generation/official/06_split_models.py

import json
import re
from code_generation.official.paths import official_paths


def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*:\s*TypeAlias)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Creates a comprehensive list of all possible header import lines for the model files."""
    return [
        "from __future__ import annotations",
        "from typing import List, Literal, TypeAlias, Annotated, Any, Union, TypedDict",
        "from uuid import UUID",
        "from enum import Enum",
        "from pydantic import BaseModel, ConfigDict, Field, RootModel",
        "",
        "from multiconn_archicad.models.base import APIModel",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_cross_file_dependencies(block: str, available_base_definitions: set[str]) -> set[str]:
    """
    Scans a command model's source code block and returns a set of dependencies
    that exist in the set of available base model names.
    """
    pattern = re.compile(r"[:\[|\(\s=]\s*([A-Z]\w*)")
    used_identifiers = set(pattern.findall(block))
    return used_identifiers.intersection(available_base_definitions)


def remove_unused_imports(content: str) -> str:
    """
    Removes unused imports from the header of the file content.
    """
    header_boundary_match = re.search(r"^(### This file is automatically generated.*?###\n)", content, re.MULTILINE)
    if not header_boundary_match:
        return content

    header_end_pos = header_boundary_match.end()
    header = content[:header_end_pos]
    body = content[header_end_pos:]

    def is_used(name: str, text: str) -> bool:
        return bool(re.search(rf"\b{name}\b", text))

    typing_imports = ["Annotated", "Any", "List", "Literal", "TypeAlias", "TypedDict", "Union"]
    used_typing = [name for name in typing_imports if is_used(name, body)]

    if "from typing import" in header:
        if used_typing:
            new_line = f"from typing import {', '.join(sorted(used_typing))}"
            header = re.sub(r"^from typing import.*$", new_line, header, flags=re.MULTILINE)
        else:
            header = re.sub(r"^from typing import.*\n", "", header, flags=re.MULTILINE)

    pydantic_imports = ["BaseModel", "ConfigDict", "Field", "RootModel"]
    used_pydantic = [name for name in pydantic_imports if is_used(name, body)]

    if "from pydantic import" in header:
        if used_pydantic:
            new_line = f"from pydantic import {', '.join(sorted(used_pydantic))}"
            header = re.sub(r"^from pydantic import.*$", new_line, header, flags=re.MULTILINE)
        else:
            header = re.sub(r"^from pydantic import.*\n", "", header, flags=re.MULTILINE)

    if "from uuid import UUID" in header and not is_used("UUID", body):
        header = re.sub(r"^from uuid import UUID\n", "", header, flags=re.MULTILINE)
    if "from enum import Enum" in header and not is_used("Enum", body):
        header = re.sub(r"^from enum import Enum\n", "", header, flags=re.MULTILINE)

    cleaned_header = re.sub(r"\n{2,}", "\n\n", header.strip())
    return cleaned_header + "\n\n" + body


def main():
    """
    Reads the single, cleaned Pydantic models file for the Official API and splits
    it into `types.py` and `commands.py`, preserving order and creating explicit imports.
    This version intelligently derives the set of base models from the file itself.
    """
    print("--- Starting Official API Model Splitting Process ---")
    official_paths.create_directories()

    try:
        # We only need the command names to distinguish commands from base types.
        with open(official_paths.COMMAND_MODELS_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"‚ùå ERROR: Could not find command name list file. Please run the schema generator first. ({e})")
        return

    try:
        content = official_paths.CLEANED_PYDANTIC_MODELS.read_text("utf-8")
    except FileNotFoundError:
        print(f"‚ùå ERROR: {official_paths.CLEANED_PYDANTIC_MODELS} not found. Please run the cleaner script first.")
        return

    # Get all definition blocks from the cleaned file.
    definitions_in_order = [
        block.strip()
        for block in re.split(r"\n\n\n", content)
        if block.strip() and not block.strip().startswith(("from ", "###", "#"))
    ]

    if not definitions_in_order:
        print("‚ùå ERROR: No model definitions found in input file.")
        return


    base_model_blocks = []
    command_model_blocks = []
    for block in definitions_in_order:
        name = get_definition_name(block)
        if name and name in command_names:
            command_model_blocks.append(block)
        else:
            base_model_blocks.append(block)

    print(f"‚úÖ Sorted models: {len(base_model_blocks)} base | {len(command_model_blocks)} command")


    # --- Write `types.py` (Base Models) ---
    print("\n‚öôÔ∏è  Processing base models (types.py)...")
    base_header_str = "\n".join(get_header_lines())
    base_body_str = "\n\n\n".join(base_model_blocks)
    base_content_uncleaned = base_header_str + "\n\n" + base_body_str
    final_base_content = remove_unused_imports(base_content_uncleaned)

    output_dir = official_paths.FINAL_PYDANTIC_TYPES.parent
    output_dir.mkdir(parents=True, exist_ok=True)
    official_paths.FINAL_PYDANTIC_TYPES.write_text(final_base_content + "\n", "utf-8")
    print(f"‚úÖ Successfully wrote {len(base_model_blocks)} definitions to {official_paths.FINAL_PYDANTIC_TYPES}")

    # --- Write `commands.py` (Command Models) ---
    print("\n‚öôÔ∏è  Processing command models (commands.py)...")
    needed_imports = set()
    # The dependency check now uses the ACCURATE set of base model names.
    all_base_model_names = {get_definition_name(b) for b in base_model_blocks if get_definition_name(b)}
    print(f"‚öôÔ∏è  Derived an accurate set of {len(all_base_model_names)} available base models for import.")
    for block in command_model_blocks:
        needed_imports.update(find_cross_file_dependencies(block, all_base_model_names))
    print(f"üîç Found {len(needed_imports)} base model dependencies required by command models.")

    command_header_str = "\n".join(get_header_lines())
    file_parts = [command_header_str]
    if needed_imports:
        import_block = "from .types import (\n    " + ",\n    ".join(sorted(list(needed_imports))) + ",\n)"
        file_parts.append(import_block)

    file_parts.extend(command_model_blocks)

    command_content_uncleaned = "\n\n\n".join(file_parts)
    final_command_content = remove_unused_imports(command_content_uncleaned)

    official_paths.FINAL_PYDANTIC_COMMANDS.write_text(final_command_content + "\n", "utf-8")
    print(f"‚úÖ Successfully wrote {len(command_model_blocks)} definitions to {official_paths.FINAL_PYDANTIC_COMMANDS}")


if __name__ == "__main__":
    main()

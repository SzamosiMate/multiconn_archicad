import json
import re
from code_generation.official.paths import official_paths


def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    # This regex handles both `class MyDict(TypedDict):` and `MyAlias = ...`
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*=)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Creates a comprehensive list of all possible header import lines for the dict files."""
    return [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, TypedDict, Union",
        "from typing_extensions import NotRequired",
        "from uuid import UUID",
        "",
        "from multiconn_archicad.dicts.extra_iems import extra_items",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_cross_file_dependencies(block: str, available_base_definitions: set[str]) -> set[str]:
    """
    Scans a command model's source code block and returns a set of dependencies
    that exist in the set of available base model names.
    """
    pattern = re.compile(r"[:\[\],|\(\s=]\s*([A-Z]\w*)")
    used_identifiers = set(pattern.findall(block))
    return used_identifiers.intersection(available_base_definitions)


def remove_unused_imports(content: str) -> str:
    """
    Removes unused imports from the header of the file content.
    """
    header_boundary_match = re.search(r"^(### This file is automatically generated.*?###\n)", content, re.MULTILINE)
    if not header_boundary_match:
        return content

    header_end_pos = header_boundary_match.end()
    header = content[:header_end_pos]
    body = content[header_end_pos:]

    def is_used(name: str, text: str) -> bool:
        return bool(re.search(rf"\b{name}\b", text))

    # Clean typing imports
    typing_imports = ["Any", "List", "Literal", "TypedDict", "Union"]
    used_typing = [name for name in typing_imports if is_used(name, body)]

    if "from typing import" in header:
        if used_typing:
            new_line = f"from typing import {', '.join(sorted(used_typing))}"
            header = re.sub(r"^from typing import.*$", new_line, header, flags=re.MULTILINE)
        else:
            header = re.sub(r"^from typing import.*\n", "", header, flags=re.MULTILINE)

    # Clean typing_extensions imports
    if "from typing_extensions import" in header and not is_used("NotRequired", body):
        header = re.sub(r"^from typing_extensions import NotRequired\n", "", header, flags=re.MULTILINE)

    # Clean uuid import
    if "from uuid import UUID" in header and not is_used("UUID", body):
        header = re.sub(r"^from uuid import UUID\n", "", header, flags=re.MULTILINE)

    cleaned_header = re.sub(r"\n{2,}", "\n\n", header.strip())
    return cleaned_header + "\n\n" + body


def main():
    """
    Reads the single, cleaned TypedDicts file for the Official API and splits
    it into `types.py` and `commands.py`, preserving order and creating explicit imports.
    """
    print("--- Starting Official API TypedDict Splitting Process ---")
    output_dir = official_paths.FINAL_TYPED_DICT_TYPES.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        with open(official_paths.COMMAND_MODELS_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"‚ùå ERROR: Could not find command name list file. Please run the schema generator first. ({e})")
        return

    try:
        content = official_paths.CLEANED_TYPED_DICTS.read_text("utf-8")
    except FileNotFoundError:
        print(f"‚ùå ERROR: {official_paths.CLEANED_TYPED_DICTS} not found. Please run the cleaner script first.")
        return

    # Get all definition blocks from the cleaned file, preserving their order.
    definitions_in_order = [
        block.strip()
        for block in re.split(r"\n\n\n", content)
        if block.strip() and not block.strip().startswith(("from ", "###", "#"))
    ]

    if not definitions_in_order:
        print("‚ùå ERROR: No definitions found in input file.")
        return

    # 1. Classify every definition into base or command lists, maintaining order.
    base_model_blocks = []
    command_model_blocks = []
    for block in definitions_in_order:
        name = get_definition_name(block)
        if name and name in command_names:
            command_model_blocks.append(block)
        else:
            base_model_blocks.append(block)

    print(f"‚úÖ Sorted definitions: {len(base_model_blocks)} base | {len(command_model_blocks)} command")

    # 2. Derive the accurate set of available base model names *directly from the classified blocks*.
    all_base_model_names = {get_definition_name(b) for b in base_model_blocks if get_definition_name(b)}

    # --- Write `types.py` (Base TypedDicts) ---
    print("\n‚öôÔ∏è  Processing base TypedDicts (types.py)...")
    base_header_str = "\n".join(get_header_lines())
    base_body_str = "\n\n\n".join(base_model_blocks)
    base_content_uncleaned = base_header_str + "\n\n" + base_body_str
    final_base_content = remove_unused_imports(base_content_uncleaned)

    official_paths.FINAL_TYPED_DICT_TYPES.write_text(final_base_content + "\n", "utf-8")
    print(f"‚úÖ Successfully wrote {len(base_model_blocks)} definitions to {official_paths.FINAL_TYPED_DICT_TYPES}")

    # --- Write `commands.py` (Command TypedDicts) ---
    print("\n‚öôÔ∏è  Processing command TypedDicts (commands.py)...")
    needed_imports = set()
    for block in command_model_blocks:
        needed_imports.update(find_cross_file_dependencies(block, all_base_model_names))
    print(f"üîç Found {len(needed_imports)} base model dependencies required by command models.")

    command_header_str = "\n".join(get_header_lines())
    file_parts = [command_header_str]
    if needed_imports:
        import_block = "from .types import (\n    " + ",\n    ".join(sorted(list(needed_imports))) + ",\n)"
        file_parts.append(import_block)

    file_parts.extend(command_model_blocks)

    command_content_uncleaned = "\n\n\n".join(file_parts)
    final_command_content = remove_unused_imports(command_content_uncleaned)

    official_paths.FINAL_TYPED_DICT_COMMANDS.write_text(final_command_content + "\n", "utf-8")
    print(
        f"‚úÖ Successfully wrote {len(command_model_blocks)} definitions to {official_paths.FINAL_TYPED_DICT_COMMANDS}"
    )


if __name__ == "__main__":
    main()

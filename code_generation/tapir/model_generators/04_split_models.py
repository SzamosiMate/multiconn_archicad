import json
import re
from code_generation.tapir.paths import paths

def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    # Corrected to handle TypeAlias syntax properly
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*:\s*TypeAlias)", block, re.MULTILINE)
    if match:
        return match.group(1) or match.group(2)
    return None


def get_header_lines() -> list[str]:
    """Creates a comprehensive list of all possible header import lines for the model files."""
    return [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, Union, TypeAlias, Annotated",
        "from uuid import UUID",
        "from enum import Enum",
        "",
        "from pydantic import BaseModel, ConfigDict, Field, RootModel",
        "",
        "### This file is automatically generated and split. Do not edit directly. ###",
    ]


def find_cross_file_dependencies(block: str, available_base_definitions: set[str]) -> set[str]:
    """
    Scans a command model's source code block and returns a set of dependencies
    that exist in the set of available base model names.
    """
    pattern = re.compile(r"[:\[|\(\s=]\s*([A-Z]\w*)")
    used_identifiers = set(pattern.findall(block))
    return used_identifiers.intersection(available_base_definitions)


def remove_unused_imports(content: str) -> str:
    """
    Removes unused imports from the header of the file content.
    """
    header_boundary_match = re.search(r"^(### This file is automatically generated.*?###\n)", content, re.MULTILINE)
    if not header_boundary_match:
        return content

    header_end_pos = header_boundary_match.end()
    header = content[:header_end_pos]
    body = content[header_end_pos:]

    def is_used(name: str, text: str) -> bool:
        return bool(re.search(rf"\b{name}\b", text))

    typing_imports = ["Annotated", "Any", "List", "Literal", "TypeAlias", "TypedDict", "Union"]
    used_typing = [name for name in typing_imports if is_used(name, body)]

    if "from typing import" in header:
        if used_typing:
            new_line = f"from typing import {', '.join(sorted(used_typing))}"
            header = re.sub(r"^from typing import.*$", new_line, header, flags=re.MULTILINE)
        else:
            header = re.sub(r"^from typing import.*\n", "", header, flags=re.MULTILINE)

    pydantic_imports = ["BaseModel", "ConfigDict", "Field", "RootModel"]
    used_pydantic = [name for name in pydantic_imports if is_used(name, body)]

    if "from pydantic import" in header:
        if used_pydantic:
            new_line = f"from pydantic import {', '.join(sorted(used_pydantic))}"
            header = re.sub(r"^from pydantic import.*$", new_line, header, flags=re.MULTILINE)
        else:
            header = re.sub(r"^from pydantic import.*\n", "", header, flags=re.MULTILINE)

    if "from uuid import UUID" in header and not is_used("UUID", body):
        header = re.sub(r"^from uuid import UUID\n", "", header, flags=re.MULTILINE)
    if "from enum import Enum" in header and not is_used("Enum", body):
        header = re.sub(r"^from enum import Enum\n", "", header, flags=re.MULTILINE)

    cleaned_header = re.sub(r"\n{2,}", "\n\n", header.strip())
    return cleaned_header + "\n\n" + body


def main():
    """
    Reads a single, ordered Pydantic models file and splits it into
    `types.py` and `commands.py`, preserving order and creating explicit imports.
    """
    print("--- Starting Model Splitting Process (with Dynamic Imports) ---")

    try:
        with open(paths.COMMAND_MODELS_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"Error: Could not find name list file. ({e})")
        return

    try:
        content = paths.CLEANED_PYDANTIC_MODELS.read_text("utf-8")
    except FileNotFoundError:
        print(f"Error: {paths.CLEANED_PYDANTIC_MODELS} not found. Please run the cleaner script first.")
        return

    definitions_in_order = [
        block.strip()
        for block in re.split(r"\n\n\n", content)
        if block.strip() and not block.strip().startswith(("from ", "###", "#"))
    ]

    if not definitions_in_order:
        print("Error: No model definitions found in input file.")
        return

    base_model_blocks = []
    command_model_blocks = []
    for block in definitions_in_order:
        name = get_definition_name(block)
        if not name:
            print(f"Warning: Could not identify name for block, placing in types.py:\n{block[:100]}...")
            base_model_blocks.append(block)
            continue

        if name in command_names:
            command_model_blocks.append(block)
        else:
            base_model_blocks.append(block)

    print(f"Sorted models: {len(base_model_blocks)} base | {len(command_model_blocks)} command")

    # --- Write `types.py` (Base Models) ---
    print("\nProcessing base models (types.py)...")
    base_header_str = "\n".join(get_header_lines())
    base_body_str = "\n\n\n".join(base_model_blocks)
    base_content_uncleaned = base_header_str + "\n\n" + base_body_str
    final_base_content = remove_unused_imports(base_content_uncleaned)

    paths.FINAL_PYDANTIC_TYPES.parent.mkdir(parents=True, exist_ok=True)
    paths.FINAL_PYDANTIC_TYPES.write_text(final_base_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(base_model_blocks)} definitions to {paths.FINAL_PYDANTIC_TYPES}")

    # --- Write `commands.py` (Command Models) ---
    print("\nProcessing command models (commands.py)...")
    base_model_names = {get_definition_name(b) for b in base_model_blocks if get_definition_name(b)}
    needed_imports = set()
    for block in command_model_blocks:
        needed_imports.update(find_cross_file_dependencies(block, base_model_names))
    print(f"Found {len(needed_imports)} base model dependencies required by command models.")

    command_header_str = "\n".join(get_header_lines())
    file_parts = [command_header_str]
    if needed_imports:
        import_block = "from .types import (\n    " + ",\n    ".join(sorted(list(needed_imports))) + ",\n)"
        file_parts.append(import_block)
    file_parts.extend(command_model_blocks)

    command_content_uncleaned = "\n\n\n".join(file_parts)
    final_command_content = remove_unused_imports(command_content_uncleaned)

    paths.FINAL_PYDANTIC_COMMANDS.parent.mkdir(parents=True, exist_ok=True)
    paths.FINAL_PYDANTIC_COMMANDS.write_text(final_command_content + "\n", "utf-8")
    print(f"✅ Successfully wrote {len(command_model_blocks)} definitions to {paths.FINAL_PYDANTIC_COMMANDS}")


if __name__ == "__main__":
    main()
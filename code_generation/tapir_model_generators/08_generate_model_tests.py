import json
import copy
from code_generation.paths import paths

# We now add the known problematic schemas that need patching.
# The `xfail` list is for problems we expect to be fixed upstream.
# The `patch_schemas` list is for problems we fix ourselves at test-time.
KNOWN_XFAILURES = {
    "GetRevisionIssuesResult": "Upstream schema has inconsistent DocumentRevision definition",
    "SetPropertyValuesOfAttributesParameters": "Upstream schema is missing AttributePropertyValues definition",
}

SCHEMAS_TO_PATCH = {"GetHotlinksResult", "GetDetailsOfElementsResult"}


def patch_schema_definitions(definitions: dict, model_name_to_test: str) -> dict:
    """
    Creates a deep copy of the definitions and applies patches to fix recursion
    or other issues that cause Hypothesis to fail or be too slow.
    """
    patched_defs = copy.deepcopy(definitions)

    if model_name_to_test == "GetHotlinksResult":
        if "Hotlink" in patched_defs and "properties" in patched_defs["Hotlink"]:
            patched_defs["Hotlink"]["properties"].pop("children", None)
        print(f"    - Applied patch to 'Hotlink' schema for {model_name_to_test} test.")

    if model_name_to_test == "GetDetailsOfElementsResult":
        # 1. Force the 'details' object to be ONLY WallDetails.
        if "TypeSpecificDetails" in patched_defs:
            patched_defs["TypeSpecificDetails"] = {"$ref": "#/$defs/WallDetails"}
            print(f"    - Patched 'TypeSpecificDetails' to be only WallDetails.")

        # 2. Force the 'type' discriminator field to be ONLY "Wall".
        item_schema = patched_defs["GetDetailsOfElementsResult"]["properties"]["detailsOfElements"]["items"]
        if "properties" in item_schema and "type" in item_schema["properties"]:
            item_schema["properties"]["type"] = {"const": "Wall"}
            print(f"    - Patched element 'type' discriminator to be 'Wall'.")

    return patched_defs


def main():
    """
    Generates a pytest file to test the instantiation of all command Pydantic models
    using property-based testing with Hypothesis.
    """
    print("--- Starting Test File Generation ---")

    try:
        with open(paths.MASTER_SCHEMA_OUTPUT, "r", encoding="utf-8") as f:
            master_schema = json.load(f)
        with open(paths.COMMAND_MODELS_NAMES_OUTPUT, "r", encoding="utf-8") as f:
            command_model_names = json.load(f)
    except FileNotFoundError as e:
        print(f"❌ Error: A required file was not found. Please run the full pipeline. ({e})")
        return

    file_header = """
# This file is automatically generated by the pipeline. Do not edit directly.

import pytest
import json
from hypothesis import given, settings
from hypothesis_jsonschema import from_schema

from multiconn_archicad.models.types import *
from multiconn_archicad.models.commands import *
"""

    test_functions = []
    original_definitions = master_schema.get("$defs", {})

    for model_name in sorted(command_model_names):
        # Determine if this test needs a patch or should be marked as xfail
        xfail_marker = ""
        if model_name in KNOWN_XFAILURES:
            reason = KNOWN_XFAILURES[model_name]
            xfail_marker = f'@pytest.mark.xfail(reason="{reason}", strict=True)'

        # Use patched definitions only for specific tests
        if model_name in SCHEMAS_TO_PATCH:
            definitions_for_test = patch_schema_definitions(original_definitions, model_name)
        else:
            definitions_for_test = original_definitions

        temp_schema_for_test = {
            "$schema": "http://json-schema.org/draft-07/schema#",
            "$defs": definitions_for_test,
            "$ref": f"#/$defs/{model_name}",
        }
        schema_as_string = json.dumps(temp_schema_for_test)

        test_function = f"""
{xfail_marker}
@settings(deadline=None)
@given(data=from_schema(json.loads(r'''{schema_as_string}''')))
def test_instantiate_{model_name}(data: dict):
    \"\"\"
    Tests that the {model_name} model can be successfully instantiated
    with valid data generated from its JSON schema.
    \"\"\"
    try:
        {model_name}.model_validate(data)
    except Exception as e:
        pytest.fail(f"Failed to instantiate {model_name} with data: {{repr(data)}}\\nError: {{e}}")

"""
        test_functions.append(test_function)

    final_content = file_header + "".join(test_functions)
    paths.GENERATED_TESTS_OUTPUT.write_text(final_content, encoding="utf-8")

    print(f"✅ Successfully generated {len(test_functions)} tests.")
    print(f"   Marked {len(KNOWN_XFAILURES)} tests as xfail.")
    print(f"   Applied patches to {len(SCHEMAS_TO_PATCH)} test schemas.")
    print(f"   Test file created at: {paths.GENERATED_TESTS_OUTPUT}")


if __name__ == "__main__":
    main()

import json
import re
from pathlib import Path

# --- Configuration ---
GENERATED_INPUT_FILE = Path("base_models.py")

# The final, separated output files
BASE_MODELS_OUTPUT = Path("base_models.py")
COMMAND_MODELS_OUTPUT = Path("command_models.py")

# The lists of names generated by the schema generator
BASE_NAMES_FILE = Path("_base_model_names.json")
COMMAND_NAMES_FILE = Path("_command_model_names.json")


def get_definition_name(block: str) -> str | None:
    """Extracts the class or TypeAlias name from a block of code."""
    # Match 'class ClassName(...):' or 'TypeName = ...'
    match = re.search(r"^(?:class\s+(\w+)|(\w+)\s*=)", block)
    if match:
        # group(1) is for class names, group(2) is for alias names
        return match.group(1) or match.group(2)
    return None


def get_file_header(is_command_file: bool = False) -> str:
    """Creates the standard header for the model files."""
    header_lines = [
        "from __future__ import annotations",
        "from typing import Any, List, Literal, Union, TypeAlias",
        "from uuid import UUID",
        "from pydantic import BaseModel, ConfigDict, Field, RootModel",
    ]
    if is_command_file:
        # Command models depend on base models, so we import them.
        header_lines.append("from .base_models import *")

    return "\n".join(header_lines)


def main():
    """
    Reads a single generated models file and splits it into
    base_models.py and command_models.py based on name lists.
    """
    print("--- Starting Model Splitting Process ---")

    # 1. Load the model name lists
    try:
        with open(BASE_NAMES_FILE) as f:
            base_names = set(json.load(f))
        with open(COMMAND_NAMES_FILE) as f:
            command_names = set(json.load(f))
    except FileNotFoundError as e:
        print(f"Error: Could not find name list file. Did you run schema_generator.py? ({e})")
        return

    # 2. Read the generated Python file
    try:
        content = GENERATED_INPUT_FILE.read_text("utf-8")
    except FileNotFoundError:
        print(f"Error: {GENERATED_INPUT_FILE} not found. Did you run generate_typed_dicts.py?")
        return

    # 3. Split the content into top-level definition blocks
    # The generator uses a consistent 'from __future__' import and newlines.
    # We can split on the blank lines that separate classes/aliases.
    definitions_raw = re.split(r'\n\n\n', content)

    # Filter out imports and empty strings
    definitions = [
        block.strip() for block in definitions_raw
        if block.strip() and not block.strip().startswith("from ") and not block.strip().startswith("# generated by")
    ]

    base_model_blocks = []
    command_model_blocks = []

    # 4. Sort each definition block into the correct category
    for block in definitions:
        name = get_definition_name(block)
        if not name:
            print(f"Warning: Could not identify name for block, placing in base_models.py:\n{block[:100]}...")
            base_model_blocks.append(block)
            continue

        # The logic: if it's a known command model, put it there. Otherwise, it's a base/common model.
        if name in command_names:
            command_model_blocks.append(block)
        else:
            base_model_blocks.append(block)

    print(f"Sorted models: {len(base_model_blocks)} base | {len(command_model_blocks)} command")

    # 5. Assemble and write the final files

    # Write base_models.py
    base_content = get_file_header() + "\n\n\n" + "\n\n\n".join(sorted(base_model_blocks))
    BASE_MODELS_OUTPUT.write_text(base_content, "utf-8")
    print(f"✅ Successfully wrote {BASE_MODELS_OUTPUT}")

    # Write command_models.py
    command_content = get_file_header(is_command_file=True) + "\n\n\n" + "\n\n\n".join(sorted(command_model_blocks))
    COMMAND_MODELS_OUTPUT.write_text(command_content, "utf-8")
    print(f"✅ Successfully wrote {COMMAND_MODELS_OUTPUT}")


if __name__ == "__main__":
    main()